import gradio as gr
print("‚è≥ ƒêang kh·ªüi ƒë·ªông... Vui l√≤ng ch·ªù...")
import soundfile as sf
import tempfile
import torch
from vieneu import VieNeuTTS, FastVieNeuTTS
import os
import sys
import time
import numpy as np
from typing import Generator, Optional, Tuple
import queue
import threading
import yaml
from vieneu_utils.core_utils import split_text_into_chunks, env_bool
from functools import lru_cache
import gc

print("‚è≥ ƒêang kh·ªüi ƒë·ªông VieNeu-TTS...")

# --- CONSTANTS & CONFIG ---
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        _config = yaml.safe_load(f) or {}
except Exception as e:
    raise RuntimeError(f"Kh√¥ng th·ªÉ ƒë·ªçc config.yaml: {e}")

BACKBONE_CONFIGS = _config.get("backbone_configs", {})
CODEC_CONFIGS = _config.get("codec_configs", {})
VOICE_SAMPLES = _config.get("voice_samples", {})

_text_settings = _config.get("text_settings", {})
MAX_CHARS_PER_CHUNK = _text_settings.get("max_chars_per_chunk", 256)
MAX_TOTAL_CHARS_STREAMING = _text_settings.get("max_total_chars_streaming", 3000)

if not BACKBONE_CONFIGS or not CODEC_CONFIGS:
    raise ValueError("config.yaml thi·∫øu backbone_configs ho·∫∑c codec_configs")
if not VOICE_SAMPLES:
    raise ValueError("config.yaml thi·∫øu voice_samples")

# --- 1. MODEL CONFIGURATION ---
# Global model instance
tts = None
current_backbone = None
current_codec = None
model_loaded = False
using_lmdeploy = True

# Cache for reference texts
_ref_text_cache = {}

def get_available_devices() -> list[str]:
    """Get list of available devices for current platform."""
    devices = ["Auto", "CPU"]

    if sys.platform == "darwin":
        # macOS - check MPS
        if torch.backends.mps.is_available():
            devices.append("MPS")
    else:
        # Windows/Linux - check CUDA
        if torch.cuda.is_available():
            devices.append("CUDA")

    return devices

def get_model_status_message() -> str:
    """Reconstruct status message from global state"""
    global model_loaded, tts, using_lmdeploy, current_backbone, current_codec
    if not model_loaded or tts is None:
        return "‚è≥ Ch∆∞a t·∫£i model."
    
    backbone_config = BACKBONE_CONFIGS.get(current_backbone, {})
    codec_config = CODEC_CONFIGS.get(current_codec, {})
    
    backend_name = "üöÄ LMDeploy (Optimized)" if using_lmdeploy else "üì¶ Standard"
    
    # We don't track the exact device strings perfectly in global state, so we estimate
    device_info = "GPU" if using_lmdeploy else "Auto"
    codec_device = "CPU" if "ONNX" in (current_codec or "") else ("GPU/MPS" if torch.cuda.is_available() or torch.backends.mps.is_available() else "CPU")
    
    preencoded_note = "\n‚ö†Ô∏è Codec ONNX kh√¥ng h·ªó tr·ª£ ch·ª©c nƒÉng clone gi·ªçng n√≥i." if codec_config.get('use_preencoded') else ""
    
    opt_info = ""
    if using_lmdeploy and hasattr(tts, 'get_optimization_stats'):
        stats = tts.get_optimization_stats()
        opt_info = (
            f"\n\nüîß T·ªëi ∆∞u h√≥a:"
            f"\n  ‚Ä¢ Triton: {'‚úÖ' if stats['triton_enabled'] else '‚ùå'}"
            f"\n  ‚Ä¢ Max Batch Size (Default): {stats.get('max_batch_size', 'N/A')}"
            f"\n  ‚Ä¢ Reference Cache: {stats['cached_references']} voices"
            f"\n  ‚Ä¢ Prefix Caching: ‚úÖ"
        )

    return (
        f"‚úÖ Model ƒë√£ t·∫£i th√†nh c√¥ng!\n\n"
        f"üîß Backend: {backend_name}\n"
        f"ü¶ú Backbone: {current_backbone}\n"
        f"üéµ Codec: {current_codec}{preencoded_note}{opt_info}"
    )

def restore_ui_state():
    """Update UI components based on persistence"""
    global model_loaded
    msg = get_model_status_message()
    return (
        msg, 
        gr.update(interactive=model_loaded), # btn_generate
        gr.update(interactive=False)         # btn_stop
    )

def should_use_lmdeploy(backbone_choice: str, device_choice: str) -> bool:
    """Determine if we should use LMDeploy backend."""
    # LMDeploy not supported on macOS
    if sys.platform == "darwin":
        return False

    if "gguf" in backbone_choice.lower():
        return False

    if device_choice == "Auto":
        has_gpu = torch.cuda.is_available()
    elif device_choice == "CUDA":
        has_gpu = torch.cuda.is_available()
    else:
        has_gpu = False

    return has_gpu

@lru_cache(maxsize=32)
def get_ref_text_cached(text_path: str) -> str:
    """Cache reference text loading"""
    with open(text_path, "r", encoding="utf-8") as f:
        return f.read()

def cleanup_gpu_memory():
    """Aggressively cleanup GPU memory"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    elif torch.backends.mps.is_available():
        torch.mps.empty_cache()
    gc.collect()

def load_model(backbone_choice: str, codec_choice: str, device_choice: str, 
               force_lmdeploy: bool):
    """Load model with optimizations and max batch size control"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy
    lmdeploy_error_reason = None
    
    yield (
        "‚è≥ ƒêang t·∫£i model v·ªõi t·ªëi ∆∞u h√≥a... L∆∞u √Ω: Qu√° tr√¨nh n√†y s·∫Ω t·ªën th·ªùi gian. Vui l√≤ng ki√™n nh·∫´n.",
        gr.update(interactive=False),
        gr.update(interactive=False),
        gr.update(interactive=False)
    )
    
    try:
        # Cleanup before loading new model
        if model_loaded and tts is not None:
            del tts
            cleanup_gpu_memory()
        
        backbone_config = BACKBONE_CONFIGS[backbone_choice]
        codec_config = CODEC_CONFIGS[codec_choice]
        
        use_lmdeploy = force_lmdeploy and should_use_lmdeploy(backbone_choice, device_choice)
        
        if use_lmdeploy:
            lmdeploy_error_reason = None
            print(f"üöÄ Using LMDeploy backend with optimizations")
            
            backbone_device = "cuda"
            
            if "ONNX" in codec_choice:
                codec_device = "cpu"
            else:
                codec_device = "cuda" if torch.cuda.is_available() else "cpu"
            
            print(f"üì¶ Loading optimized model...")
            print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")
            print(f"   Triton: Enabled")
            
            try:
                tts = FastVieNeuTTS(
                    backbone_repo=backbone_config["repo"],
                    backbone_device=backbone_device,
                    codec_repo=codec_config["repo"],
                    codec_device=codec_device,
                    memory_util=0.3,
                    tp=1,
                    enable_prefix_caching=True,
                    enable_triton=True,
                )
                using_lmdeploy = True
                
                # Pre-cache voice references
                print("üìù Pre-caching voice references...")
                for voice_name, voice_info in VOICE_SAMPLES.items():
                    audio_path = voice_info["audio"]
                    text_path = voice_info["text"]
                    if os.path.exists(audio_path) and os.path.exists(text_path):
                        ref_text = get_ref_text_cached(text_path)
                        tts.get_cached_reference(voice_name, audio_path, ref_text)
                print(f"   ‚úÖ Cached {len(VOICE_SAMPLES)} voices")
                
            except Exception as e:
                import traceback
                traceback.print_exc()
                
                error_str = str(e)
                if "$env:CUDA_PATH" in error_str:
                    lmdeploy_error_reason = "Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng CUDA_PATH. Vui l√≤ng c√†i ƒë·∫∑t NVIDIA GPU Computing Toolkit."
                else:
                    lmdeploy_error_reason = f"{error_str}"
                
                yield (
                    f"‚ö†Ô∏è LMDeploy Init Error: {lmdeploy_error_reason}. ƒêang loading model v·ªõi backend m·∫∑c ƒë·ªãnh - t·ªëc ƒë·ªô ch·∫≠m h∆°n so v·ªõi lmdeploy...",
                    gr.update(interactive=False),
                    gr.update(interactive=False)
                )
                time.sleep(1)
                use_lmdeploy = False
                using_lmdeploy = False
        
        if not use_lmdeploy:
            print(f"üì¶ Using original backend")

            if device_choice == "Auto":
                if "gguf" in backbone_choice.lower():
                    # GGUF: uses Metal on Mac, CUDA on Windows/Linux
                    if sys.platform == "darwin":
                        backbone_device = "gpu"  # llama-cpp-python uses Metal
                    else:
                        backbone_device = "gpu" if torch.cuda.is_available() else "cpu"
                else:
                    # PyTorch model
                    if sys.platform == "darwin":
                        backbone_device = "mps" if torch.backends.mps.is_available() else "cpu"
                    else:
                        backbone_device = "cuda" if torch.cuda.is_available() else "cpu"

                # Codec device
                if "ONNX" in codec_choice:
                    codec_device = "cpu"
                elif sys.platform == "darwin":
                    codec_device = "mps" if torch.backends.mps.is_available() else "cpu"
                else:
                    codec_device = "cuda" if torch.cuda.is_available() else "cpu"

            elif device_choice == "MPS":
                backbone_device = "mps"
                codec_device = "mps" if "ONNX" not in codec_choice else "cpu"

            else:
                backbone_device = device_choice.lower()
                codec_device = device_choice.lower()

                if "ONNX" in codec_choice:
                    codec_device = "cpu"

            if "gguf" in backbone_choice.lower() and backbone_device == "cuda":
                backbone_device = "gpu"
            
            print(f"üì¶ Loading model...")
            print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")
            
            tts = VieNeuTTS(
                backbone_repo=backbone_config["repo"],
                backbone_device=backbone_device,
                codec_repo=codec_config["repo"],
                codec_device=codec_device
            )
            using_lmdeploy = False
        
        current_backbone = backbone_choice
        current_codec = codec_choice
        model_loaded = True
        
        # Success message with optimization info
        backend_name = "üöÄ LMDeploy (Optimized)" if using_lmdeploy else "üì¶ Standard"
        device_info = "cuda" if use_lmdeploy else (backbone_device if not use_lmdeploy else "N/A")
        
        streaming_support = "‚úÖ C√≥" if backbone_config['supports_streaming'] else "‚ùå Kh√¥ng"
        preencoded_note = "\n‚ö†Ô∏è Codec n√†y c·∫ßn s·ª≠ d·ª•ng pre-encoded codes (.pt files)" if codec_config['use_preencoded'] else ""
        
        opt_info = ""
        if using_lmdeploy and hasattr(tts, 'get_optimization_stats'):
            stats = tts.get_optimization_stats()
            opt_info = (
                f"\n\nüîß T·ªëi ∆∞u h√≥a:"
                f"\n  ‚Ä¢ Triton: {'‚úÖ' if stats['triton_enabled'] else '‚ùå'}"
                f"\n  ‚Ä¢ Max Batch Size (Default): {stats.get('max_batch_size', 'N/A')}"
                f"\n  ‚Ä¢ Reference Cache: {stats['cached_references']} voices"
                f"\n  ‚Ä¢ Prefix Caching: ‚úÖ"
            )
        
        warning_msg = ""
        if lmdeploy_error_reason:
             warning_msg = (
                 f"\n\n‚ö†Ô∏è **C·∫£nh b√°o:** Kh√¥ng th·ªÉ k√≠ch ho·∫°t LMDeploy (Optimized Backend) do l·ªói sau:\n"
                 f"üëâ {lmdeploy_error_reason}\n"
                 f"üí° H·ªá th·ªëng ƒë√£ t·ª± ƒë·ªông chuy·ªÉn v·ªÅ ch·∫ø ƒë·ªô Standard (ch·∫≠m h∆°n)."
             )

        success_msg = get_model_status_message()
        if warning_msg:
            success_msg += warning_msg
            
        yield (
            success_msg,
            gr.update(interactive=True), # btn_generate
            gr.update(interactive=True), # btn_load
            gr.update(interactive=False) # btn_stop
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        model_loaded = False
        using_lmdeploy = False

        if "$env:CUDA_PATH" in str(e):
            yield (
                "‚ùå L·ªói khi t·∫£i model: Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng CUDA_PATH. Vui l√≤ng c√†i ƒë·∫∑t NVIDIA GPU Computing Toolkit (https://developer.nvidia.com/cuda/toolkit)",
                gr.update(interactive=False),
                gr.update(interactive=True),
                gr.update(interactive=False)
            )
        else: 
            yield (
                f"‚ùå L·ªói khi t·∫£i model: {str(e)}",
                gr.update(interactive=False),
                gr.update(interactive=True),
                gr.update(interactive=False)
            )


# --- 2. DATA & HELPERS ---
GGUF_ALLOWED_VOICES = [
    "B√¨nh (nam mi·ªÅn B·∫Øc)",
    "Tuy√™n (nam mi·ªÅn B·∫Øc)",
    "Vƒ©nh (nam mi·ªÅn Nam)",
    "ƒêoan (n·ªØ mi·ªÅn Nam)",
    "Ly (n·ªØ mi·ªÅn B·∫Øc)",
    "Ng·ªçc (n·ªØ mi·ªÅn B·∫Øc)",
]

def get_voice_options(backbone_choice: str):
    """Filter voice options: GGUF only shows the 4 allowed voices."""
    if "gguf" in backbone_choice.lower():
        return [v for v in GGUF_ALLOWED_VOICES if v in VOICE_SAMPLES]
    return list(VOICE_SAMPLES.keys())

def update_voice_dropdown(backbone_choice: str, current_voice: str):
    options = get_voice_options(backbone_choice)
    new_value = current_voice if current_voice in options else (options[0] if options else None)
    return gr.update(choices=options, value=new_value)

# --- 3. CORE LOGIC FUNCTIONS ---
def load_reference_info(voice_choice: str) -> Tuple[Optional[str], str]:
    """Load reference audio and text with caching"""
    if voice_choice in VOICE_SAMPLES:
        audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        text_path = VOICE_SAMPLES[voice_choice]["text"]
        try:
            if os.path.exists(text_path):
                ref_text = get_ref_text_cached(text_path)
                return audio_path, ref_text
            else:
                return audio_path, "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file text m·∫´u."
        except Exception as e:
            return None, f"‚ùå L·ªói: {str(e)}"
    return None, ""

def synthesize_speech(text: str, voice_choice: str, custom_audio, custom_text: str, 
                     mode_tab: str, generation_mode: str, use_batch: bool, max_batch_size_run: int,
                     lora_repo_id: str, lora_hf_token: str, lora_audio, lora_text: str):
    """Synthesis with optimization support, max batch size control, and LoRA adapter support"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy
    
    if not model_loaded or tts is None:
        yield None, "‚ö†Ô∏è Vui l√≤ng t·∫£i model tr∆∞·ªõc!"
        return
    
    if not text or text.strip() == "":
        yield None, "‚ö†Ô∏è Vui l√≤ng nh·∫≠p vƒÉn b·∫£n!"
        return
    
    raw_text = text.strip()
    
    codec_config = CODEC_CONFIGS[current_codec]
    use_preencoded = codec_config['use_preencoded']
    
    # Handle LoRA mode
    lora_loaded = False
    if hasattr(tts, '_lora_loaded') and tts._lora_loaded:
        lora_loaded = True

    # If not in LoRA mode but a LoRA is loaded, unload it now to prevent conflicts
    if mode_tab != "lora_mode" and lora_loaded:
        yield None, "üîÑ ƒêang d·ªçn d·∫πp LoRA adapter ƒë·ªÉ quay v·ªÅ model g·ªëc..."
        try:
            tts.unload_lora_adapter()
            lora_loaded = False
        except Exception as e:
            print(f"Error unloading LoRA: {e}")

    if mode_tab == "lora_mode":
        # Check if using LMDeploy backend
        if using_lmdeploy:
            yield None, (
                "‚ùå LoRA adapter kh√¥ng h·ªó tr·ª£ LMDeploy backend!\n\n"
                "üí° Gi·∫£i ph√°p:\n"
                "1. B·ªè tick 'üöÄ Optimize with LMDeploy' ·ªü ph·∫ßn c·∫•u h√¨nh\n"
                "2. Click 'üîÑ T·∫£i Model' l·∫°i\n"
                "3. Quay l·∫°i tab LoRA v√† th·ª≠ l·∫°i\n\n"
                "üìù L∆∞u √Ω: Khi d√πng LoRA, t·ªëc ƒë·ªô s·∫Ω ch·∫≠m h∆°n LMDeploy. Ho·∫∑c b·∫°n c√≥ th·ªÉ c√¢n nh·∫Øc merge LoRA v√†o model g·ªëc r·ªìi d√πng LMDeploy ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô."
            )
            return
        
        if not lora_repo_id or not lora_repo_id.strip():
            yield None, "‚ö†Ô∏è Vui l√≤ng nh·∫≠p HuggingFace Repo ID c·ªßa LoRA adapter!"
            return
        
        if not lora_audio or not lora_text or not lora_text.strip():
            yield None, "‚ö†Ô∏è Thi·∫øu Audio ho·∫∑c Text reference t·ª´ t·∫≠p train c·ªßa LoRA!"
            return
        
        # Only load if not already loaded or if repo changed
        current_lora = getattr(tts, '_current_lora_repo', None)
        if not lora_loaded or current_lora != lora_repo_id:
            yield None, f"üì¶ ƒêang t·∫£i LoRA adapter t·ª´ {lora_repo_id}..."
            try:
                # Use the new load_lora_adapter method from VieNeuTTS class
                hf_token = lora_hf_token.strip() if lora_hf_token and lora_hf_token.strip() else None
                tts.load_lora_adapter(lora_repo_id, hf_token=hf_token)
                lora_loaded = True
                yield None, "‚úÖ LoRA adapter loaded! ƒêang x·ª≠ l√Ω..."
            except NotImplementedError as e:
                yield None, f"‚ùå {str(e)}\n\nVui l√≤ng ch·ªçn backbone PyTorch (VieNeu-TTS ho·∫∑c VieNeu-TTS-0.3B GPU), kh√¥ng d√πng GGUF."
                return
            except RuntimeError as e:
                error_msg = str(e)
                # Detect backbone mismatch
                suggestion = ""
                if "size mismatch" in error_msg.lower() or "shape" in error_msg.lower():
                    current_backbone_name = BACKBONE_CONFIGS[current_backbone]['repo']
                    suggestion = (
                        f"\n\nüí° **C√≥ th·ªÉ do backbone kh√¥ng kh·ªõp!**\n"
                        f"- Backbone hi·ªán t·∫°i: `{current_backbone_name}`\n"
                        f"- H√£y ki·ªÉm tra LoRA repo c·ªßa b·∫°n ƒë∆∞·ª£c train tr√™n model n√†o\n"
                        f"- N·∫øu train tr√™n VieNeu-TTS-0.3B ‚Üí Ch·ªçn **VieNeu-TTS-0.3B (GPU)**\n"
                        f"- N·∫øu train tr√™n VieNeu-TTS (0.5B) ‚Üí Ch·ªçn **VieNeu-TTS (GPU)**"
                    )
                yield None, f"‚ùå L·ªói khi t·∫£i LoRA adapter: {error_msg}{suggestion}"
                return
            except Exception as e:
                import traceback
                traceback.print_exc()
                yield None, f"‚ùå L·ªói khi t·∫£i LoRA adapter: {str(e)}\n\nKi·ªÉm tra:\n- Repo ID c√≥ ƒë√∫ng kh√¥ng?\n- Token c√≥ h·ª£p l·ªá kh√¥ng (n·∫øu private)?"
                return
        else:
            yield None, f"‚úÖ S·ª≠ d·ª•ng LoRA ƒë√£ load: {lora_repo_id}"
        
        # Use LoRA reference audio/text
        ref_audio_path = lora_audio
        ref_text_raw = lora_text
        ref_codes_path = None
        
    # Setup Reference (non-LoRA modes)
    elif mode_tab == "custom_mode":
        if custom_audio is None or not custom_text:
            yield None, "‚ö†Ô∏è Thi·∫øu Audio ho·∫∑c Text m·∫´u custom."
            return
        ref_audio_path = custom_audio
        ref_text_raw = custom_text
        ref_codes_path = None
    else:
        if voice_choice not in VOICE_SAMPLES:
            yield None, "‚ö†Ô∏è Vui l√≤ng ch·ªçn gi·ªçng m·∫´u."
            return
        ref_audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        text_path = VOICE_SAMPLES[voice_choice]["text"]
        ref_codes_path = VOICE_SAMPLES[voice_choice]["codes"]
        
        if not os.path.exists(ref_audio_path):
            yield None, "‚ùå Kh√¥ng t√¨m th·∫•y file audio m·∫´u."
            return
        
        ref_text_raw = get_ref_text_cached(text_path)
    
    yield None, "üìÑ ƒêang x·ª≠ l√Ω Reference..."
    
    # Encode or get cached reference
    try:
        if use_preencoded and ref_codes_path and os.path.exists(ref_codes_path):
            ref_codes = torch.load(ref_codes_path, map_location="cpu", weights_only=True)
        else:
            # Use cached reference if available (LMDeploy only)
            if using_lmdeploy and hasattr(tts, 'get_cached_reference') and mode_tab == "preset_mode":
                ref_codes = tts.get_cached_reference(voice_choice, ref_audio_path, ref_text_raw)
            else:
                ref_codes = tts.encode_reference(ref_audio_path)
        
        if isinstance(ref_codes, torch.Tensor):
            ref_codes = ref_codes.cpu().numpy()
    except Exception as e:
        yield None, f"‚ùå L·ªói x·ª≠ l√Ω reference: {e}"
        return
    
    text_chunks = split_text_into_chunks(raw_text, max_chars=MAX_CHARS_PER_CHUNK)
    total_chunks = len(text_chunks)
    
    # === STANDARD MODE ===
    if generation_mode == "Standard (M·ªôt l·∫ßn)":
        backend_name = "LMDeploy" if using_lmdeploy else "Standard"
        batch_info = " (Batch Mode)" if use_batch and using_lmdeploy and total_chunks > 1 else ""
        
        # Show batch size info
        batch_size_info = ""
        if use_batch and using_lmdeploy and hasattr(tts, 'max_batch_size'):
            batch_size_info = f" [Max batch: {tts.max_batch_size}]"
        
        yield None, f"üöÄ B·∫Øt ƒë·∫ßu t·ªïng h·ª£p {backend_name}{batch_info}{batch_size_info} ({total_chunks} ƒëo·∫°n)..."
        
        all_audio_segments = []
        sr = 24000
        silence_pad = np.zeros(int(sr * 0.15), dtype=np.float32)
        
        start_time = time.time()
        
        try:
            # Use batch processing if enabled and using LMDeploy
            if use_batch and using_lmdeploy and hasattr(tts, 'infer_batch') and total_chunks > 1:
                # Show how many mini-batches will be processed
                num_batches = (total_chunks + max_batch_size_run - 1) // max_batch_size_run
                
                yield None, f"‚ö° X·ª≠ l√Ω {num_batches} mini-batch(es) (max {max_batch_size_run} ƒëo·∫°n/batch)..."
                
                chunk_wavs = tts.infer_batch(text_chunks, ref_codes, ref_text_raw, max_batch_size=max_batch_size_run)
                
                for i, chunk_wav in enumerate(chunk_wavs):
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_audio_segments.append(chunk_wav)
                        if i < total_chunks - 1:
                            all_audio_segments.append(silence_pad)
            else:
                # Sequential processing
                for i, chunk in enumerate(text_chunks):
                    yield None, f"‚è≥ ƒêang x·ª≠ l√Ω ƒëo·∫°n {i+1}/{total_chunks}..."
                    
                    chunk_wav = tts.infer(chunk, ref_codes, ref_text_raw)
                    
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_audio_segments.append(chunk_wav)
                        if i < total_chunks - 1:
                            all_audio_segments.append(silence_pad)
            
            if not all_audio_segments:
                yield None, "‚ùå Kh√¥ng sinh ƒë∆∞·ª£c audio n√†o."
                return
            
            yield None, "üíæ ƒêang gh√©p file v√† l∆∞u..."
            
            final_wav = np.concatenate(all_audio_segments)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                output_path = tmp.name
            
            process_time = time.time() - start_time
            backend_info = f" (Backend: {'LMDeploy üöÄ' if using_lmdeploy else 'Standard üì¶'})"
            speed_info = f", T·ªëc ƒë·ªô: {len(final_wav)/sr/process_time:.2f}x realtime" if process_time > 0 else ""
            
            # LoRA info
            lora_info = f" [LoRA: {lora_repo_id}]" if lora_loaded else ""
            
            yield output_path, f"‚úÖ Ho√†n t·∫•t! (Th·ªùi gian: {process_time:.2f}s{speed_info}){backend_info}{lora_info}"
            
            # Cleanup memory
            if using_lmdeploy and hasattr(tts, 'cleanup_memory'):
                tts.cleanup_memory()
            
            cleanup_gpu_memory()
            
        except torch.cuda.OutOfMemoryError as e:
            cleanup_gpu_memory()
            yield None, (
                f"‚ùå GPU h·∫øt VRAM! H√£y th·ª≠:\n"
                f"‚Ä¢ Gi·∫£m Max Batch Size (hi·ªán t·∫°i: {tts.max_batch_size if hasattr(tts, 'max_batch_size') else 'N/A'})\n"
                f"‚Ä¢ Gi·∫£m ƒë·ªô d√†i vƒÉn b·∫£n\n\n"
                f"Chi ti·∫øt: {str(e)}"
            )
            return
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            cleanup_gpu_memory()
            yield None, f"‚ùå L·ªói Standard Mode: {str(e)}"
            return
    
    # === STREAMING MODE ===
    else:
        sr = 24000
        crossfade_samples = int(sr * 0.03)
        audio_queue = queue.Queue(maxsize=100)
        PRE_BUFFER_SIZE = 3
        
        end_event = threading.Event()
        error_event = threading.Event()
        error_msg = ""
        
        def producer_thread():
            nonlocal error_msg
            try:
                previous_tail = None
                
                for i, chunk_text in enumerate(text_chunks):
                    stream_gen = tts.infer_stream(chunk_text, ref_codes, ref_text_raw)
                    
                    for part_idx, audio_part in enumerate(stream_gen):
                        if audio_part is None or len(audio_part) == 0:
                            continue
                        
                        if previous_tail is not None and len(previous_tail) > 0:
                            overlap = min(len(previous_tail), len(audio_part), crossfade_samples)
                            if overlap > 0:
                                fade_out = np.linspace(1.0, 0.0, overlap, dtype=np.float32)
                                fade_in = np.linspace(0.0, 1.0, overlap, dtype=np.float32)
                                
                                blended = (audio_part[:overlap] * fade_in + 
                                         previous_tail[-overlap:] * fade_out)
                                
                                processed = np.concatenate([
                                    previous_tail[:-overlap] if len(previous_tail) > overlap else np.array([]),
                                    blended,
                                    audio_part[overlap:]
                                ])
                            else:
                                processed = np.concatenate([previous_tail, audio_part])
                            
                            tail_size = min(crossfade_samples, len(processed))
                            previous_tail = processed[-tail_size:].copy()
                            output_chunk = processed[:-tail_size] if len(processed) > tail_size else processed
                        else:
                            tail_size = min(crossfade_samples, len(audio_part))
                            previous_tail = audio_part[-tail_size:].copy()
                            output_chunk = audio_part[:-tail_size] if len(audio_part) > tail_size else audio_part
                        
                        if len(output_chunk) > 0:
                            audio_queue.put((sr, output_chunk))
                
                if previous_tail is not None and len(previous_tail) > 0:
                    audio_queue.put((sr, previous_tail))
                    
            except Exception as e:
                import traceback
                traceback.print_exc()
                error_msg = str(e)
                error_event.set()
            finally:
                end_event.set()
                audio_queue.put(None)
        
        threading.Thread(target=producer_thread, daemon=True).start()
        
        yield (sr, np.zeros(int(sr * 0.05))), "üìÑ ƒêang buffering..."
        
        pre_buffer = []
        while len(pre_buffer) < PRE_BUFFER_SIZE:
            try:
                item = audio_queue.get(timeout=5.0)
                if item is None:
                    break
                pre_buffer.append(item)
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    return
                break
        
        full_audio_buffer = []
        backend_info = "üöÄ LMDeploy" if using_lmdeploy else "üì¶ Standard"
        for sr, audio_data in pre_buffer:
            full_audio_buffer.append(audio_data)
            yield (sr, audio_data), f"üîä ƒêang ph√°t ({backend_info})..."
        
        while True:
            try:
                item = audio_queue.get(timeout=0.05)
                if item is None:
                    break
                sr, audio_data = item
                full_audio_buffer.append(audio_data)
                yield (sr, audio_data), f"üîä ƒêang ph√°t ({backend_info})..."
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    break
                if end_event.is_set() and audio_queue.empty():
                    break
                continue
        
        if full_audio_buffer:
            final_wav = np.concatenate(full_audio_buffer)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                
                lora_info = f" [LoRA: {lora_repo_id}]" if lora_loaded else ""
                yield tmp.name, f"‚úÖ Ho√†n t·∫•t Streaming! ({backend_info}){lora_info}"
            
            # Cleanup memory
            if using_lmdeploy and hasattr(tts, 'cleanup_memory'):
                tts.cleanup_memory()
            
            cleanup_gpu_memory()


# --- 4. UI SETUP ---
theme = gr.themes.Soft(
    primary_hue="indigo",
    secondary_hue="cyan",
    neutral_hue="slate",
    font=[gr.themes.GoogleFont('Inter'), 'ui-sans-serif', 'system-ui'],
).set(
    button_primary_background_fill="linear-gradient(90deg, #6366f1 0%, #0ea5e9 100%)",
    button_primary_background_fill_hover="linear-gradient(90deg, #4f46e5 0%, #0284c7 100%)",
)

css = """
.container { max-width: 1400px; margin: auto; }
.header-box {
    text-align: center;
    margin-bottom: 25px;
    padding: 25px;
    background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
    border-radius: 12px;
    color: white !important;
}
.header-title {
    font-size: 2.5rem;
    font-weight: 800;
    color: white !important;
}
.gradient-text {
    background: -webkit-linear-gradient(45deg, #60A5FA, #22D3EE);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}
.header-icon {
    color: white;
}
.status-box {
    font-weight: 500;
    border: 1px solid rgba(99, 102, 241, 0.1);
    background: rgba(99, 102, 241, 0.03);
    border-radius: 8px;
}
.status-box textarea {
    text-align: center;
    font-family: inherit;
}
.model-card-content {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    align-items: center;
    gap: 15px;
    font-size: 0.9rem;
    text-align: center;
    color: white !important;
}
.model-card-item {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 6px;
    color: white !important;
}
.model-card-item strong {
    color: white !important;
}
.model-card-item span {
    color: white !important;
}
.model-card-link {
    color: #60A5FA;
    text-decoration: none;
    font-weight: 500;
    transition: color 0.2s;
}
.model-card-link:hover {
    color: #22D3EE;
    text-decoration: underline;
}
.warning-banner {
    background-color: #fffbeb;
    border: 1px solid #fef3c7;
    border-radius: 12px;
    padding: 16px;
    margin-bottom: 20px;
}
.warning-banner-title {
    color: #92400e;
    font-weight: 700;
    font-size: 1.1rem;
    display: flex;
    align-items: center;
    gap: 8px;
    margin-bottom: 12px;
}
.warning-banner-grid {
    display: flex;
    gap: 15px;
    flex-wrap: wrap;
}
.warning-banner-item {
    flex: 1;
    min-width: 240px;
    background: #fef3c7;
    padding: 12px;
    border-radius: 8px;
    border: 1px solid #fde68a;
}
.warning-banner-item strong {
    color: #b45309;
    display: block;
    margin-bottom: 4px;
    font-size: 0.95rem;
}
.warning-banner-content {
    color: #78350f;
    font-size: 0.9rem;
    line-height: 1.5;
}
.warning-banner-content b {
    color: #451a03;
    background: rgba(251, 191, 36, 0.2);
    padding: 1px 4px;
    border-radius: 4px;
}
"""

EXAMPLES_LIST = [
    ["V·ªÅ mi·ªÅn T√¢y kh√¥ng ch·ªâ ƒë·ªÉ ng·∫Øm nh√¨n s√¥ng n∆∞·ªõc h·ªØu t√¨nh, m√† c√≤n ƒë·ªÉ c·∫£m nh·∫≠n t·∫•m ch√¢n t√¨nh c·ªßa ng∆∞·ªùi d√¢n n∆°i ƒë√¢y.", "Vƒ©nh (nam mi·ªÅn Nam)"],
    ["H√† N·ªôi nh·ªØng ng√†y v√†o thu mang m·ªôt v·∫ª ƒë·∫πp tr·∫ßm m·∫∑c v√† c·ªï k√≠nh ƒë·∫øn l·∫° th∆∞·ªùng.", "B√¨nh (nam mi·ªÅn B·∫Øc)"],
]

with gr.Blocks(theme=theme, css=css, title="VieNeu-TTS") as demo:
    with gr.Column(elem_classes="container"):
        gr.HTML("""
<div class="header-box">
    <h1 class="header-title">
        <span class="header-icon">ü¶ú</span>
        <span class="gradient-text">VieNeu-TTS Studio</span>
    </h1>
    <div class="model-card-content">
        <div class="model-card-item">
            <strong>Models:</strong>
            <a href="https://huggingface.co/pnnbao-ump/VieNeu-TTS" target="_blank" class="model-card-link">VieNeu-TTS</a>
            <span>‚Ä¢</span>
            <a href="https://huggingface.co/pnnbao-ump/VieNeu-TTS-0.3B" target="_blank" class="model-card-link">VieNeu-TTS-0.3B</a>
        </div>
        <div class="model-card-item">
            <strong>Repository:</strong>
            <a href="https://github.com/pnnbao97/VieNeu-TTS" target="_blank" class="model-card-link">GitHub</a>
        </div>
        <div class="model-card-item">
            <strong>T√°c gi·∫£:</strong>
            <a href="https://www.facebook.com/bao.phamnguyenngoc.5" target="_blank" class="model-card-link">Ph·∫°m Nguy·ªÖn Ng·ªçc B·∫£o</a>
        </div>
    </div>
</div>
        """)
        
        # --- CONFIGURATION ---
        with gr.Group():
            with gr.Row():
                backbone_select = gr.Dropdown(
                    list(BACKBONE_CONFIGS.keys()), 
                    value="VieNeu-TTS (GPU)", 
                    label="ü¶ú Backbone"
                )
                codec_select = gr.Dropdown(list(CODEC_CONFIGS.keys()), value="NeuCodec (Distill)", label="üéµ Codec")
                device_choice = gr.Radio(get_available_devices(), value="Auto", label="üñ•Ô∏è Device")
            
            with gr.Row():
                use_lmdeploy_cb = gr.Checkbox(
                    value=True, 
                    label="üöÄ Optimize with LMDeploy (Khuy√™n d√πng cho NVIDIA GPU)",
                    info="Tick n·∫øu b·∫°n d√πng GPU ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô t·ªïng h·ª£p ƒë√°ng k·ªÉ."
                )
            
            gr.HTML("""
            <div class="warning-banner">
                <div class="warning-banner-title">
                    ü¶ú G·ª£i √Ω t·ªëi ∆∞u hi·ªáu nƒÉng
                </div>
                <div class="warning-banner-grid">
                    <div class="warning-banner-item">
                        <strong>üê¢ H·ªá m√°y CPU</strong>
                        <div class="warning-banner-content">
                            S·ª≠ d·ª•ng <b>VieNeu-TTS-0.3B-q4-gguf</b> ƒë·ªÉ ƒë·∫°t t·ªëc ƒë·ªô x·ª≠ l√Ω nhanh nh·∫•t. N·∫øu ∆∞u ti√™n ƒë·ªô ch√≠nh x√°c th√¨ d√πng <b>VieNeu-TTS-0.3B-q8-gguf</b>.
                        </div>
                    </div>
                    <div class="warning-banner-item">
                        <strong>üêÜ H·ªá m√°y GPU</strong>
                        <div class="warning-banner-content">
                            Ch·ªçn <b>VieNeu-TTS-0.3B (GPU)</b> ƒë·ªÉ x2 t·ªëc ƒë·ªô (ƒë·ªô ch√≠nh x√°c ~95% b·∫£n g·ªëc).
                        </div>
                    </div>
                </div>
            </div>
            """)

            btn_load = gr.Button("üîÑ T·∫£i Model", variant="primary")
            model_status = gr.Markdown("‚è≥ Ch∆∞a t·∫£i model.")
        
        with gr.Row(elem_classes="container"):
            # --- INPUT ---
            with gr.Column(scale=3):
                text_input = gr.Textbox(
                    label=f"VƒÉn b·∫£n",
                    lines=4,
                    value="H√† N·ªôi, tr√°i tim c·ªßa Vi·ªát Nam, l√† m·ªôt th√†nh ph·ªë ng√†n nƒÉm vƒÉn hi·∫øn v·ªõi b·ªÅ d√†y l·ªãch s·ª≠ v√† vƒÉn h√≥a ƒë·ªôc ƒë√°o. B∆∞·ªõc ch√¢n tr√™n nh·ªØng con ph·ªë c·ªï k√≠nh quanh H·ªì Ho√†n Ki·∫øm, du kh√°ch nh∆∞ ƒë∆∞·ª£c du h√†nh ng∆∞·ª£c th·ªùi gian, chi√™m ng∆∞·ª°ng ki·∫øn tr√∫c Ph√°p c·ªï ƒëi·ªÉn h√≤a quy·ªán v·ªõi n√©t ki·∫øn tr√∫c truy·ªÅn th·ªëng Vi·ªát Nam. M·ªói con ph·ªë trong khu ph·ªë c·ªï mang m·ªôt t√™n g·ªçi ƒë·∫∑c tr∆∞ng, ph·∫£n √°nh ngh·ªÅ th·ªß c√¥ng truy·ªÅn th·ªëng t·ª´ng th·ªãnh h√†nh n∆°i ƒë√¢y nh∆∞ ph·ªë H√†ng B·∫°c, H√†ng ƒê√†o, H√†ng M√£. ·∫®m th·ª±c H√† N·ªôi c≈©ng l√† m·ªôt ƒëi·ªÉm nh·∫•n ƒë·∫∑c bi·ªát, t·ª´ t√¥ ph·ªü n√≥ng h·ªïi bu·ªïi s√°ng, b√∫n ch·∫£ th∆°m l·ª´ng tr∆∞a h√®, ƒë·∫øn ch√® Th√°i ng·ªçt ng√†o chi·ªÅu thu. Nh·ªØng m√≥n ƒÉn d√¢n d√£ n√†y ƒë√£ tr·ªü th√†nh bi·ªÉu t∆∞·ª£ng c·ªßa vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát, ƒë∆∞·ª£c c·∫£ th·∫ø gi·ªõi y√™u m·∫øn. Ng∆∞·ªùi H√† N·ªôi n·ªïi ti·∫øng v·ªõi t√≠nh c√°ch hi·ªÅn h√≤a, l·ªãch thi·ªáp nh∆∞ng c≈©ng r·∫•t c·∫ßu to√†n trong t·ª´ng chi ti·∫øt nh·ªè, t·ª´ c√°ch pha tr√† sen cho ƒë·∫øn c√°ch ch·ªçn hoa sen t√¢y ƒë·ªÉ th∆∞·ªüng tr√†.",
                )
                
                with gr.Tabs() as tabs:
                    with gr.TabItem("üë§ Preset", id="preset_mode") as tab_preset:
                        initial_voices = get_voice_options("VieNeu-TTS (GPU)")
                        default_voice = initial_voices[0] if initial_voices else None
                        voice_select = gr.Dropdown(initial_voices, value=default_voice, label="Gi·ªçng m·∫´u")
                    
                    with gr.TabItem("ü¶ú Voice Cloning", id="custom_mode") as tab_custom:
                        custom_audio = gr.Audio(label="Audio gi·ªçng m·∫´u (3-5 gi√¢y) (.wav)", type="filepath")
                        custom_text = gr.Textbox(label="N·ªôi dung audio m·∫´u - vui l√≤ng g√µ ƒë√∫ng n·ªôi dung c·ªßa audio m·∫´u - k·ªÉ c·∫£ d·∫•u c√¢u v√¨ model r·∫•t nh·∫°y c·∫£m v·ªõi d·∫•u c√¢u (.,?!)")
                        gr.Examples(
                            examples=[
                                [os.path.join("examples", "audio_ref", "example.wav"), "V√≠ d·ª• 2. T√≠nh trung b√¨nh c·ªßa d√£y s·ªë."],
                                [os.path.join("examples", "audio_ref", "example_2.wav"), "Tr√™n th·ª±c t·∫ø, c√°c nghi ng·ªù ƒë√£ b·∫Øt ƒë·∫ßu xu·∫•t hi·ªán."],
                                [os.path.join("examples", "audio_ref", "example_3.wav"), "C·∫≠u c√≥ nh√¨n th·∫•y kh√¥ng?"],
                                [os.path.join("examples", "audio_ref", "example_4.wav"), "T·∫øt l√† d·ªãp m·ªçi ng∆∞·ªùi h√°o h·ª©c ƒë√≥n ch√†o m·ªôt nƒÉm m·ªõi v·ªõi nhi·ªÅu hy v·ªçng v√† mong ∆∞·ªõc."]
                            ],
                            inputs=[custom_audio, custom_text],
                            label="V√≠ d·ª• m·∫´u ƒë·ªÉ th·ª≠ nghi·ªám clone gi·ªçng"
                        )
                        
                        gr.Markdown("""
                        **üí° M·∫πo nh·ªè:** N·∫øu k·∫øt qu·∫£ Zero-shot Voice Cloning ch∆∞a nh∆∞ √Ω, b·∫°n h√£y c√¢n nh·∫Øc **Finetune (LoRA)** ƒë·ªÉ ƒë·∫°t ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t. 
                        H∆∞·ªõng d·∫´n chi ti·∫øt c√≥ t·∫°i file: `finetune/README.md` ho·∫∑c xem tr√™n [GitHub](https://github.com/pnnbao97/VieNeu-TTS/tree/main/finetune).
                        """)
                    
                    with gr.TabItem("üéØ LoRA Adapter", id="lora_mode") as tab_lora:
                        gr.Markdown("""
                        ### üéì S·ª≠ d·ª•ng LoRA Adapter ƒë√£ fine-tune
                        
                        T·∫£i LoRA adapter t·ª´ HuggingFace ƒë·ªÉ s·ª≠ d·ª•ng gi·ªçng n√≥i ƒë√£ ƒë∆∞·ª£c fine-tune.
                        
                        ‚ö†Ô∏è **QUAN TR·ªåNG - Y√™u c·∫ßu:**
                        
                        **1. Backbone ph·∫£i kh·ªõp:**
                        - N·∫øu train LoRA tr√™n **VieNeu-TTS-0.3B** ‚Üí Ph·∫£i ch·ªçn backbone **VieNeu-TTS-0.3B (GPU)** ·ªü tr√™n
                        - N·∫øu train LoRA tr√™n **VieNeu-TTS** (0.5B) ‚Üí Ph·∫£i ch·ªçn backbone **VieNeu-TTS (GPU)** ·ªü tr√™n
                        
                        **2. KH√îNG d√πng v·ªõi:**
                        - ‚ùå GGUF models (ch·ªâ h·ªó tr·ª£ PyTorch backbone)
                        - ‚ùå LMDeploy optimization (b·ªè tick "üöÄ Optimize with LMDeploy")
                        
                        üí° Ki·ªÉm tra model base trong file `adapter_config.json` c·ªßa LoRA repo ƒë·ªÉ bi·∫øt model n√†o ƒë∆∞·ª£c d√πng.
                        """)
                        
                        with gr.Row():
                            lora_repo_id = gr.Textbox(
                                label="ü§ó HuggingFace Repo ID",
                                placeholder="vd: pnnbao-ump/VieNeu-TTS-0.3B-lora-ngoc-huyen",
                                value="pnnbao-ump/VieNeu-TTS-0.3B-lora-ngoc-huyen",
                                info="Nh·∫≠p repo ID c·ªßa LoRA adapter tr√™n HuggingFace"
                            )
                            lora_hf_token = gr.Textbox(
                                label="üîë HF Token (n·∫øu repo private)",
                                placeholder="ƒê·ªÉ tr·ªëng n·∫øu repo public",
                                type="password",
                                info="Token ƒë·ªÉ truy c·∫≠p repo private"
                            )
                        
                        gr.Markdown("**üì§ Upload Audio m·∫´u t·ª´ t·∫≠p train c·ªßa LoRA**")
                        lora_audio = gr.Audio(
                            label="Audio reference (ph·∫£i l√† audio t·ª´ t·∫≠p train c·ªßa LoRA)",
                            type="filepath",
                            value=os.path.join("examples", "audio_ref", "example_ngoc_huyen.wav")
                        )
                        lora_text = gr.Textbox(
                            label="Text t∆∞∆°ng ·ª©ng v·ªõi audio reference",
                            placeholder="Nh·∫≠p ch√≠nh x√°c n·ªôi dung c·ªßa audio reference...",
                            value="T√°c ph·∫©m d·ª± thi b·∫£o ƒë·∫£m t√≠nh khoa h·ªçc, t√≠nh ƒë·∫£ng, t√≠nh chi·∫øn ƒë·∫•u, t√≠nh ƒë·ªãnh h∆∞·ªõng."
                        )

                        gr.Examples(
                            examples=[
                                [
                                    "pnnbao-ump/VieNeu-TTS-0.3B-lora-ngoc-huyen",
                                    "", # hf token
                                    os.path.join("examples", "audio_ref", "example_ngoc_huyen.wav"),
                                    "T√°c ph·∫©m d·ª± thi b·∫£o ƒë·∫£m t√≠nh khoa h·ªçc, t√≠nh ƒë·∫£ng, t√≠nh chi·∫øn ƒë·∫•u, t√≠nh ƒë·ªãnh h∆∞·ªõng."
                                ]
                            ],
                            inputs=[lora_repo_id, lora_hf_token, lora_audio, lora_text],
                            label="V√≠ d·ª• m·∫´u LoRA Ng·ªçc Huy·ªÅn"
                        )

                
                generation_mode = gr.Radio(
                    ["Standard (M·ªôt l·∫ßn)"],
                    value="Standard (M·ªôt l·∫ßn)",
                    label="Ch·∫ø ƒë·ªô sinh"
                )
                with gr.Row():
                    use_batch = gr.Checkbox(
                        value=True, 
                        label="‚ö° Batch Processing",
                        info="X·ª≠ l√Ω nhi·ªÅu ƒëo·∫°n c√πng l√∫c (ch·ªâ √°p d·ª•ng khi s·ª≠ d·ª•ng GPU v√† ƒë√£ c√†i ƒë·∫∑t LMDeploy)"
                    )
                    max_batch_size_run = gr.Slider(
                        minimum=1, 
                        maximum=16, 
                        value=4, 
                        step=1, 
                        label="üìä Batch Size (Generation)",
                        info="S·ªë l∆∞·ª£ng ƒëo·∫°n vƒÉn b·∫£n x·ª≠ l√Ω c√πng l√∫c. Gi√° tr·ªã cao = nhanh h∆°n nh∆∞ng t·ªën VRAM h∆°n. Gi·∫£m xu·ªëng n·∫øu g·∫∑p l·ªói Out of Memory."
                    )
                
                # State to track current mode (replaces unreliable Textbox/Tabs input)
                current_mode_state = gr.State("preset_mode")
                
                with gr.Row():
                    btn_generate = gr.Button("üéµ B·∫Øt ƒë·∫ßu", variant="primary", scale=2, interactive=False)
                    btn_stop = gr.Button("‚èπÔ∏è D·ª´ng", variant="stop", scale=1, interactive=False)
            
            # --- OUTPUT ---
            with gr.Column(scale=2):
                audio_output = gr.Audio(
                    label="K·∫øt qu·∫£",
                    type="filepath",
                    autoplay=True
                )
                status_output = gr.Textbox(
                    label="Tr·∫°ng th√°i", 
                    elem_classes="status-box",
                    lines=2,
                    max_lines=10,
                    show_copy_button=True
                )
                gr.Markdown("<div style='text-align: center; color: #64748b; font-size: 0.8rem;'>üîí Audio ƒë∆∞·ª£c ƒë√≥ng d·∫•u b·∫£n quy·ªÅn ·∫©n (Watermarker) ƒë·ªÉ b·∫£o m·∫≠t v√† ƒë·ªãnh danh AI.</div>")
        
        # # --- EVENT HANDLERS ---
        # def update_info(backbone: str) -> str:
        #     return f"Streaming: {'‚úÖ' if BACKBONE_CONFIGS[backbone]['supports_streaming'] else '‚ùå'}"
        
        # backbone_select.change(update_info, backbone_select, model_status)
        backbone_select.change(update_voice_dropdown, [backbone_select, voice_select], voice_select)
        
        # Handler to show/hide Voice Cloning tab
        def on_codec_change(codec: str):
            is_onnx = "onnx" in codec.lower()
            # If switching to ONNX and we are on custom mode, switch back to preset
            return gr.update(visible=not is_onnx), gr.update(selected="preset_mode" if is_onnx else None)
        
        codec_select.change(
            on_codec_change, 
            inputs=[codec_select], 
            outputs=[tab_custom, tabs]
        )
        
        # Bind tab events to update state
        tab_preset.select(lambda: "preset_mode", outputs=current_mode_state)
        tab_custom.select(lambda: "custom_mode", outputs=current_mode_state)
        tab_lora.select(lambda: "lora_mode", outputs=current_mode_state)
        
        btn_load.click(
            fn=load_model,
            inputs=[backbone_select, codec_select, device_choice, use_lmdeploy_cb],
            outputs=[model_status, btn_generate, btn_load, btn_stop]
        )
        
        generate_event = btn_generate.click(
            fn=synthesize_speech,
            inputs=[text_input, voice_select, custom_audio, custom_text, current_mode_state, 
                    generation_mode, use_batch, max_batch_size_run,
                    lora_repo_id, lora_hf_token, lora_audio, lora_text],
            outputs=[audio_output, status_output]
        )
        
        # When generation starts, enable stop button
        btn_generate.click(lambda: gr.update(interactive=True), outputs=btn_stop)
        # When generation ends/stops, disable stop button
        generate_event.then(lambda: gr.update(interactive=False), outputs=btn_stop)
        
        btn_stop.click(fn=None, cancels=[generate_event])
        btn_stop.click(lambda: (None, "‚èπÔ∏è ƒê√£ d·ª´ng t·∫°o gi·ªçng n√≥i."), outputs=[audio_output, status_output])
        btn_stop.click(lambda: gr.update(interactive=False), outputs=btn_stop)

        # Persistence: Restore UI state on load
        demo.load(
            fn=restore_ui_state,
            outputs=[model_status, btn_generate, btn_stop]
        )

if __name__ == "__main__":
    # Cho ph√©p override t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (h·ªØu √≠ch cho Docker)
    server_name = os.getenv("GRADIO_SERVER_NAME", "127.0.0.1")
    server_port = int(os.getenv("GRADIO_SERVER_PORT", "7860"))

    # Check running in Colab
    is_on_colab = os.getenv("COLAB_RELEASE_TAG") is not None

    # Default:
    # - Colab: share=True (convenient)
    # - Docker/local: share=False (safe)
    share = env_bool("GRADIO_SHARE", default=is_on_colab)
    #
    # If server_name is "0.0.0.0" and GRADIO_SHARE is not set, disable sharing
    if server_name == "0.0.0.0" and os.getenv("GRADIO_SHARE") is None:
        share = False

    demo.queue().launch(server_name=server_name, server_port=server_port, share=True)
